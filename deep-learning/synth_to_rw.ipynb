{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpCdm5MTVLke"
      },
      "source": [
        "## Setup\n",
        "### Project setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAJzLPaqVLkj"
      },
      "outputs": [],
      "source": [
        "if run_init:\n",
        "    %pip install -U pip\n",
        "    !if  [ ! -d \"deep-learning-project\" ] ; then git clone https://github.com/albertsgarde/deep-learning-project.git; fi\n",
        "    !cd deep-learning-project && git reset --hard && git pull && git checkout Fix_guitar_json && git pull\n",
        "    !source deep-learning-project/setup.sh deep-learning-project\n",
        "    import os\n",
        "    os.chdir(\"deep-learning-project/deep-learning\")\n",
        "run_init = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGYtl2AsVLkm"
      },
      "outputs": [],
      "source": [
        "run_init = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ixbGgM3VLkn"
      },
      "outputs": [],
      "source": [
        "run_init = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByS9iXtkVLko"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h_vOCYPVLkp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as display\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as nn_func\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import audio_samples_py as aus\n",
        "\n",
        "import utils.plots as plots\n",
        "import utils.criterion as chord_criterion\n",
        "import utils.utils as utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z74dCplZVLkq"
      },
      "source": [
        "### Device setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hg-wbljVLkr"
      },
      "outputs": [],
      "source": [
        "device, use_cuda = utils.setup_device(use_cuda_if_possible = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OetAsjTMVLks"
      },
      "source": [
        "## Data\n",
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2BDxTBAVLku"
      },
      "outputs": [],
      "source": [
        "SAMPLE_LENGTH = 1024\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GmdJSKP1odx"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Synth parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SAMPLE_LENGTH = 1024\n",
        "BATCH_SIZE = 64\n",
        "SEED = 1 # Generates different data if changed. Useful to ensure that a result isn't a fluke.\n",
        "\n",
        "possible_chord_types = [i for i in range(aus.num_chord_types())]\n",
        "octave_parameters = aus.OctaveParameters(add_root_octave_probability=0.5,\n",
        "        add_other_octave_probability=0.3)\n",
        "parameters = aus.DataParameters(num_samples=SAMPLE_LENGTH, octave_parameters=octave_parameters, min_frequency=50, max_frequency=2000, min_frequency_std_dev=0.5, max_frequency_std_dev=3., possible_chord_types=possible_chord_types) \\\n",
        "    .add_sine(probability=0.5, amplitude_range=(0.1,0.2)) \\\n",
        "    .add_saw(probability=0.5, amplitude_range=(0.1, 0.2)) \\\n",
        "    .add_pulse(probability=0.5, amplitude_range=(0.1, 0.2), duty_cycle_range=(0.1, 0.9)) \\\n",
        "    .add_triangle(probability=0.5, amplitude_range=(0.1, 0.2)) \\\n",
        "    .add_noise(probability=1, amplitude_range=(0.001, 0.04)) \\\n",
        "    .apply_distortion(probability=0.5, power_range=(0.1, 20)) \\\n",
        "    .apply_normalization(probability=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C96SLNm-1odx"
      },
      "source": [
        "### Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSzxYUoh1odx"
      },
      "outputs": [],
      "source": [
        "def init_rw_data(path, label_to_target, validation_size: float, batch_size: int):\n",
        "    assert validation_size >= 0, f\"validation size must be non-negative. validation_size={validation_size}\"\n",
        "    assert validation_size <= 1, f\"validation size must be no greater than 1. validation_size={validation_size}\"\n",
        "    assert batch_size > 0, f\"batch_size must be positive. batch_size={batch_size}\"\n",
        "\n",
        "    data_loader_params = {\"batch_size\": batch_size, \"collate_fn\": utils.custom_collate, \"shuffle\": True, \"drop_last\": True}\n",
        "\n",
        "    # Not a mistake. Just an artifact of how random_partition works.\n",
        "    training_data, validation_data = aus.load_data_set(path).random_partition(1-validation_size)\n",
        "\n",
        "    training_data = utils.AudioRwDataSet(training_data, label_to_target)\n",
        "    validation_data = utils.AudioRwDataSet(validation_data, label_to_target)\n",
        "\n",
        "    training_loader = torch.utils.data.DataLoader(training_data, **data_loader_params)\n",
        "    validation_loader = torch.utils.data.DataLoader(validation_data, **data_loader_params)\n",
        "\n",
        "    return training_loader, validation_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISgSLVdDVLkx"
      },
      "outputs": [],
      "source": [
        "def label_to_target(label):\n",
        "    target = np.zeros(aus.num_chord_types() + 12, dtype=np.float32)\n",
        "    target[label.chord_type()] = 1\n",
        "    target[aus.num_chord_types() + label.note()] = 1\n",
        "    return target\n",
        "    \n",
        "synth_training_parameters, synth_training_loader, synth_validation_parameters, synth_validation_loader = utils.init_synth_data(parameters, label_to_target, SEED, BATCH_SIZE)\n",
        "\n",
        "rw_training_loader, rw_validation_loader = init_rw_data(\"data/short_guitar_samples/\", label_to_target, 0.2, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-TuEI2LVLk0"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvMod(nn.Module):\n",
        "    \"\"\"Convolutional module containing a list of convolutional layers with ReLU activation and optional batch normalization and dropout.\"\"\"\n",
        "    def __init__(self, name, input_size, depth, channels, kernel_sizes, strides, paddings, poolings, batchnorms, dropouts):\n",
        "        super(ConvMod, self).__init__()\n",
        "\n",
        "        # Ensure that all parameter lists are long enough.\n",
        "        assert len(channels) >= depth\n",
        "        assert len(kernel_sizes) >= depth\n",
        "        assert len(strides) >= depth\n",
        "        assert len(paddings) >= depth\n",
        "        assert len(poolings) >= depth\n",
        "        assert len(batchnorms) >= depth\n",
        "        assert len(dropouts) >= depth\n",
        "        for kernel_size in kernel_sizes:\n",
        "            assert kernel_size % 2 == 1, \"Only odd kernel sizes are supported.\"\n",
        "        for dropout in dropouts:\n",
        "            assert 0 <= dropout and dropout <= 1, \"Dropout must be between 0 and 1.\"\n",
        "\n",
        "        # Calculate the size of the output of each convolutional layer (for each channel).\n",
        "        conv_size = []\n",
        "        for i in range(depth):\n",
        "            conv_dim_reduction = kernel_sizes[i]-1-paddings[i]*2\n",
        "            assert (input_size - conv_dim_reduction) % (strides[i]*poolings[i]) == 0\n",
        "            conv_size.append(int((input_size - conv_dim_reduction)/strides[i]/poolings[i]))\n",
        "            input_size = conv_size[i]\n",
        "        for i in range(depth):\n",
        "            print(f\"{name} layer {i} output: size={conv_size[i]} channels={channels[i]}\")\n",
        "\n",
        "        # Calculate total size of the output of the convolutional layers.\n",
        "        self.output_size = conv_size[-1]*channels[-1]\n",
        "        \n",
        "        self.module_list = nn.ModuleList()\n",
        "        in_channels = 1\n",
        "        for i in range(depth):\n",
        "            conv = nn.Conv1d(in_channels=in_channels, out_channels=channels[i], kernel_size=kernel_sizes[i], stride=strides[i], padding=paddings[i])\n",
        "            in_channels = channels[i]\n",
        "            pool = nn.MaxPool1d(poolings[i])\n",
        "            batchnorm = nn.BatchNorm1d(channels[i]) if (batchnorms[i]) else nn.Identity()\n",
        "            dropout = nn.Dropout(p=dropouts[i])\n",
        "\n",
        "            self.module_list.append(nn.ModuleList([conv, pool, batchnorm, dropout]))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for layer in self.module_list:\n",
        "            conv = layer[0]\n",
        "            pool = layer[1]\n",
        "            batchnorm = layer[2]\n",
        "            dropout = layer[3]\n",
        "            x = conv(x)\n",
        "            x = pool(x)\n",
        "            x = nn_func.relu(x)\n",
        "            x = batchnorm(x)\n",
        "            x = dropout(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def total_output_size(self):\n",
        "        \"\"\" Returns the size of the output of the convolutional layers. \"\"\"\n",
        "        return self.output_size\n",
        "\n",
        "class LinMod(nn.Module):\n",
        "    def __init__(self, name, in_features, depth, features, batchnorms, dropouts):\n",
        "        super(LinMod, self).__init__()\n",
        "\n",
        "        # Ensure that all parameter lists are long enough.\n",
        "        assert in_features > 0\n",
        "        assert depth > 0\n",
        "        assert len(features) >= depth\n",
        "        assert len(batchnorms) >= depth\n",
        "        assert len(dropouts) >= depth\n",
        "        for dropout in dropouts:\n",
        "            assert 0 <= dropout and dropout <= 1, \"Dropout must be between 0 and 1.\"\n",
        "        for i in range(depth):\n",
        "            print(f\"{name} layer {i} output: size={features[i]}\")\n",
        "\n",
        "        self.module_list = nn.ModuleList()\n",
        "        for i in range(depth):\n",
        "            lin = nn.Linear(in_features=in_features, out_features=features[i])\n",
        "            in_features = features[i]\n",
        "            batchnorm = nn.BatchNorm1d(features[i]) if (batchnorms[i]) else nn.Identity()\n",
        "            dropout = nn.Dropout(p=dropouts[i])\n",
        "\n",
        "            self.module_list.append(nn.ModuleList([lin, batchnorm, dropout]))\n",
        "        \n",
        "        self.out_features = in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.module_list:\n",
        "            lin = layer[0]\n",
        "            batchnorm = layer[1]\n",
        "            dropout = layer[2]\n",
        "            x = lin(x)\n",
        "            x = nn_func.relu(x)\n",
        "            x = batchnorm(x)\n",
        "            x = dropout(x)\n",
        "            \n",
        "        return x\n",
        "\n",
        "    def total_output_size(self):\n",
        "        return self.out_features\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, signal_mod, fft_mod, lin_mod, num_chord_types):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.signal_mod = signal_mod\n",
        "\n",
        "        self.fft_mod = fft_mod\n",
        "        \n",
        "        self.lin = lin_mod\n",
        "\n",
        "        self.num_chord_types = num_chord_types\n",
        "\n",
        "        self.lin_out = nn.Linear(in_features=self.lin.total_output_size(), out_features=num_chord_types+12)\n",
        "        self.chord_type_softmax = nn.Softmax(dim=1)\n",
        "        self.chord_tone_softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "        \n",
        "    def forward(self, signal, fft):\n",
        "        features = []\n",
        "\n",
        "        \n",
        "        if self.signal_mod is not None:\n",
        "            signal_features = signal.unsqueeze(1)\n",
        "            signal_features = self.signal_mod(signal_features).flatten(1)\n",
        "            features.append(signal_features)\n",
        "        \n",
        "        if self.fft_mod is not None:\n",
        "            fft_features = fft.unsqueeze(1) \n",
        "            fft_features = self.fft_mod(fft_features).flatten(1)\n",
        "            features.append(fft_features)\n",
        "\n",
        "        features = torch.cat(features, dim=1)\n",
        "\n",
        "        final_features = self.lin(features)\n",
        "\n",
        "        logits = self.lin_out(final_features)\n",
        "\n",
        "        chord_type_probabilities = self.chord_type_softmax(logits[:,:self.num_chord_types])\n",
        "        chord_tone_probabilities = self.chord_tone_softmax(logits[:,self.num_chord_types:])\n",
        "            \n",
        "        return torch.cat([chord_type_probabilities, chord_tone_probabilities], dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "signal_conv_depth = 6\n",
        "signal_kernel_sizes = [9, 9, 9, 9, 9, 9]\n",
        "signal_conv = ConvMod(\n",
        "    \"signal_conv\",\n",
        "    input_size = SAMPLE_LENGTH,\n",
        "    depth = signal_conv_depth,\n",
        "    channels = [64 for _ in range(signal_conv_depth)], \n",
        "    kernel_sizes = signal_kernel_sizes, \n",
        "    strides = [1 for _ in range(signal_conv_depth)], \n",
        "    paddings = [int((kernel_size - 1)/2) for kernel_size in signal_kernel_sizes], \n",
        "    poolings = [2,2,2,2,1,1], \n",
        "    batchnorms = [True for _ in range(signal_conv_depth)], \n",
        "    dropouts = [0.2 for _ in range(signal_conv_depth)])\n",
        "#signal_conv = None\n",
        "\n",
        "fft_conv_depth = 6\n",
        "fft_kernel_sizes = [9, 9, 9, 9, 9, 9]\n",
        "fft_conv = ConvMod(\n",
        "    \"fft_conv\",\n",
        "    input_size = SAMPLE_LENGTH,\n",
        "    depth = fft_conv_depth,\n",
        "    channels = [16 for _ in range(fft_conv_depth)], \n",
        "    kernel_sizes = fft_kernel_sizes, \n",
        "    strides = [1 for _ in range(fft_conv_depth)], \n",
        "    paddings = [int((kernel_size - 1)/2) for kernel_size in fft_kernel_sizes], \n",
        "    poolings = [2,2,2,2,1,1], \n",
        "    batchnorms = [True for _ in range(fft_conv_depth)], \n",
        "    dropouts = [0.2 for _ in range(fft_conv_depth)])\n",
        "#fft_conv = None\n",
        "\n",
        "lin_depth = 4\n",
        "lin_mod = LinMod(\n",
        "    \"lin\",\n",
        "    in_features=(signal_conv.total_output_size() if signal_conv else 0) + (fft_conv.total_output_size() if fft_conv else 0),\n",
        "    depth = lin_depth,\n",
        "    features = [1024, 1024, 512, 256],\n",
        "    batchnorms = [True for _ in range(lin_depth)],\n",
        "    dropouts = [0.2 for _ in range(lin_depth)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAYW93NKVLk3"
      },
      "source": [
        "### Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aC9CSpPVLk3"
      },
      "outputs": [],
      "source": [
        "new_model = True\n",
        "pre_train = False\n",
        "\n",
        "if new_model:\n",
        "    assert signal_conv or fft_conv, \"Either signal or fft must be used.\"\n",
        "\n",
        "    net = Net(signal_conv, fft_conv, lin_mod, aus.num_chord_types())\n",
        "else:\n",
        "    model_path = \"C:/Users/alber/Downloads/model_200000.pt\"\n",
        "\n",
        "    net = torch.jit.load(model_path)\n",
        "\n",
        "\n",
        "if use_cuda:\n",
        "    net.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33PC3H1tVLk4"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYwDiU2u1od0"
      },
      "source": [
        "### Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO34ceKr1od1"
      },
      "outputs": [],
      "source": [
        "def type_accuracy(output, target):\n",
        "    output_chord_type = np.argmax(output[:aus.num_chord_types()])\n",
        "    target_chord_type = np.argmax(target[:aus.num_chord_types()])\n",
        "    return 1 if output_chord_type == target_chord_type else 0\n",
        "\n",
        "def tone_accuracy(output, target):\n",
        "    output_chord_tone = np.argmax(output[aus.num_chord_types():])\n",
        "    target_chord_tone = np.argmax(target[aus.num_chord_types():])\n",
        "    return 1 if output_chord_tone == target_chord_tone else 0\n",
        "\n",
        "def total_accuracy(output, target):\n",
        "    return type_accuracy(output, target) * tone_accuracy(output, target)\n",
        "\n",
        "eval_funcs = [\n",
        "    {\n",
        "        \"label\": \"Type Accuracy\",\n",
        "        \"ylim\": (0,1),\n",
        "        \"func\": lambda output, target, label: type_accuracy(output, target)\n",
        "    },\n",
        "    {\n",
        "        \"label\": \"Tone Accuracy\",\n",
        "        \"ylim\": (0,1),\n",
        "        \"func\": lambda output, target, label: tone_accuracy(output, target)\n",
        "    },\n",
        "    {\n",
        "        \"label\": \"Accuracy\",\n",
        "        \"ylim\": (0,1),\n",
        "        \"func\": lambda output, target, label: total_accuracy(output, target)\n",
        "    }\n",
        "]   \n",
        "\n",
        "manual_test_funcs = {\n",
        "    \"Target type\": lambda output, target, label: aus.chord_type_name(np.argmax(target[:aus.num_chord_types()])),\n",
        "    \"Output type\": lambda output, target, label: aus.chord_type_name(np.argmax(output[:aus.num_chord_types()])),\n",
        "    \"Target tone\": lambda output, target, label: np.argmax(target[aus.num_chord_types():]),\n",
        "    \"Output tone\": lambda output, target, label: np.argmax(output[aus.num_chord_types():]),\n",
        "    \"Frequency\": lambda output, target, label: label.frequency(),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oMSXT4M1od1"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLbmywdRVLk5"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLXdnp1sVLk5"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()  \n",
        "\n",
        "# weight_decay is equal to L2 regularization\n",
        "optimizer = optim.AdamW(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Synth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if new_model and pre_train:\n",
        "    NUM_BATCHES = 12000\n",
        "    EVAL_EVERY = 500\n",
        "    LOG_TRAIN_EVERY = 5\n",
        "    NUM_VALIDATION_BATCHES = 10\n",
        "    LOG_EVERY = EVAL_EVERY\n",
        "    SAVE_MODEL_EVERY = 5000\n",
        "\n",
        "    synth_error_tracker = utils.ErrorTracker(criterion, eval_funcs, NUM_VALIDATION_BATCHES)\n",
        "    rw_error_tracker = utils.ErrorTracker(criterion, eval_funcs, NUM_VALIDATION_BATCHES)\n",
        "\n",
        "    net.train()\n",
        "    for i, (signal, fft, target, label) in enumerate(itertools.islice(synth_training_loader, NUM_BATCHES+1)):\n",
        "        if i%EVAL_EVERY == 0:\n",
        "            synth_error_tracker.validation_update(i, net, synth_validation_loader)\n",
        "            rw_error_tracker.validation_update(i, net, rw_validation_loader)\n",
        "\n",
        "        signal = utils.to_torch(signal)\n",
        "        fft = utils.to_torch(fft)\n",
        "        target = utils.to_torch(target)\n",
        "        output = net(signal, fft)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        if i%LOG_TRAIN_EVERY == 0:\n",
        "            synth_error_tracker.training_update(i, output, target, label, loss)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % LOG_EVERY == 0:\n",
        "            display.clear_output(wait=True)\n",
        "            plots.plot_history(synth_error_tracker, NUM_BATCHES, eval_funcs)\n",
        "            plots.plot_history(rw_error_tracker, NUM_BATCHES, eval_funcs)\n",
        "            #plots.frequency_plot(net, synth_validation_loader, 20, eval_funcs, 30)\n",
        "            plt.show()\n",
        "        print(f\"Batch: {i}\", end=\"\\r\")\n",
        "\n",
        "\n",
        "    val_loss, [val_type_accuracy, val_tone_accuracy, val_accuracy] = utils.test_net(net, synth_validation_loader, criterion, NUM_VALIDATION_BATCHES, eval_funcs)\n",
        "\n",
        "    print(f\"Loss={val_loss}, Type Accuracy={val_type_accuracy:.3f}, Tone Accuracy={val_tone_accuracy:.3f}, Accuracy={val_accuracy:.3f}\")\n",
        "\n",
        "    utils.manual_test(net, synth_validation_loader, 5, manual_test_funcs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Real World"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbqU-FdsVLk6"
      },
      "outputs": [],
      "source": [
        "NUM_BATCHES = 2000\n",
        "EVAL_EVERY = 200\n",
        "LOG_TRAIN_EVERY = 10\n",
        "NUM_VALIDATION_BATCHES = 10\n",
        "LOG_EVERY = EVAL_EVERY\n",
        "\n",
        "def save_model(path, net, batch_num):\n",
        "    Path(path).mkdir(parents=True, exist_ok=True)\n",
        "    path = f\"{path}/model_{batch_num}.pt\"\n",
        "    model_scripted = torch.jit.script(net)\n",
        "    model_scripted.save(path)\n",
        "\n",
        "error_tracker = utils.ErrorTracker(criterion, eval_funcs, NUM_VALIDATION_BATCHES)\n",
        "\n",
        "net.train()\n",
        "for i, (signal, fft, target, label) in enumerate(itertools.islice(utils.cycle_data_loader(rw_training_loader), NUM_BATCHES+1)):\n",
        "    if i%EVAL_EVERY == 0:\n",
        "        error_tracker.validation_update(i, net, rw_validation_loader)\n",
        "\n",
        "    signal = utils.to_torch(signal)\n",
        "    fft = utils.to_torch(fft)\n",
        "    target = utils.to_torch(target)\n",
        "    output = net(signal, fft)\n",
        "    loss = criterion(output, target)\n",
        "\n",
        "    if i%LOG_TRAIN_EVERY == 0:\n",
        "        error_tracker.training_update(i, output, target, label, loss)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if i % LOG_EVERY == 0:\n",
        "        display.clear_output(wait=True)\n",
        "        plots.plot_history(error_tracker, NUM_BATCHES, eval_funcs)\n",
        "        plt.show()\n",
        "    print(f\"Batch: {i}\", end=\"\\r\")\n",
        "\n",
        "net.eval()\n",
        "val_loss, [val_type_accuracy, val_tone_accuracy, val_accuracy] = utils.test_net(net, rw_validation_loader, criterion, NUM_VALIDATION_BATCHES, eval_funcs)\n",
        "\n",
        "print(f\"Loss={val_loss}, Type Accuracy={val_type_accuracy:.3f}, Tone Accuracy={val_tone_accuracy:.3f}, Accuracy={val_accuracy:.3f}\")\n",
        "\n",
        "utils.manual_test(net, rw_validation_loader, 5, manual_test_funcs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EpCdm5MTVLke",
        "ByS9iXtkVLko",
        "Z74dCplZVLkq",
        "OetAsjTMVLks",
        "r-TuEI2LVLk0",
        "cnk9srl9_o6E",
        "MLAnanJZVLk0",
        "xAYW93NKVLk3",
        "33PC3H1tVLk4",
        "gYwDiU2u1od0",
        "6oMSXT4M1od1",
        "-jCFxF-w1od2"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 ('torch-notebook')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "34fdfbbea481bd85da6d3a89cefc0eac7829bd3d33c7e2764c66b35aab7d912a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
