{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpCdm5MTVLke"
      },
      "source": [
        "## Setup\n",
        "### Project setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAJzLPaqVLkj"
      },
      "outputs": [],
      "source": [
        "if run_init:\n",
        "    %pip install -U pip\n",
        "    !if  [ ! -d \"deep-learning-project\" ] ; then git clone https://github.com/albertsgarde/deep-learning-project.git; fi\n",
        "    !cd deep-learning-project && git reset --hard && git pull\n",
        "    !source deep-learning-project/setup.sh deep-learning-project\n",
        "run_init = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGYtl2AsVLkm"
      },
      "outputs": [],
      "source": [
        "run_init = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ixbGgM3VLkn"
      },
      "outputs": [],
      "source": [
        "run_init = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByS9iXtkVLko"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h_vOCYPVLkp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as display\n",
        "\n",
        "import numpy as np\n",
        "import itertools\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as nn_func\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import audio_samples_py as aus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z74dCplZVLkq"
      },
      "source": [
        "### Device setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hg-wbljVLkr"
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available() and True\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(\"Running GPU.\") if use_cuda else print(\"No GPU available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OetAsjTMVLks"
      },
      "source": [
        "## Data\n",
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2BDxTBAVLku"
      },
      "outputs": [],
      "source": [
        "SAMPLE_LENGTH = 256\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "parameters = aus.DataParameters(num_samples=SAMPLE_LENGTH).add_sine((0.5,0.75))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq5XoGNBVLkv"
      },
      "source": [
        "### Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxMriVnzVLkw"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "class AudioDataSet(torch.utils.data.Dataset):\n",
        "    def __init__(self, parameters: aus.DataParameters):\n",
        "         self.parameters = parameters\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.iinfo(np.int64).max\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        data_point = self.parameters.generate_at_index(index)\n",
        "        return data_point.samples(), data_point.audio().fft(), torch.tensor([data_point.frequency_map()]).unsqueeze(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISgSLVdDVLkx"
      },
      "outputs": [],
      "source": [
        "data_loader_params = {\"batch_size\": BATCH_SIZE}\n",
        "\n",
        "training_parameters = parameters.with_seed_offset(0)\n",
        "training_loader = torch.utils.data.DataLoader(AudioDataSet(training_parameters), **data_loader_params)\n",
        "validation_parameters = parameters.with_seed_offset(1)\n",
        "validation_loader = torch.utils.data.DataLoader(AudioDataSet(validation_parameters), **data_loader_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_s9IW5uVLky"
      },
      "outputs": [],
      "source": [
        "def get_numpy(x):\n",
        "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
        "    if use_cuda:\n",
        "        return x.cpu().data.numpy()\n",
        "    return x.data.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-TuEI2LVLk0"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvMod(nn.Module):\n",
        "    def __init__(self, depth, channels, kernel_sizes, strides, paddings, poolings, batch_norms, dropouts):\n",
        "        super(ConvMod, self).__init__()\n",
        "\n",
        "        # Ensure that all parameter lists are the right length.\n",
        "        assert len(channels) >= depth\n",
        "        assert len(kernel_sizes) >= depth\n",
        "        assert len(strides) >= depth\n",
        "        assert len(paddings) >= depth\n",
        "        assert len(poolings) >= depth\n",
        "        assert len(batch_norms) >= depth\n",
        "        assert len(dropouts) >= depth\n",
        "        for kernel_size in kernel_sizes:\n",
        "            assert kernel_size % 2 == 1, \"Only odd kernel sizes are supported.\"\n",
        "        for dropout in dropouts:\n",
        "            assert 0 <= dropout and dropout <= 1, \"Dropout must be between 0 and 1.\"\n",
        "\n",
        "        # Calculate the size of the output of each convolutional layer (for each channel).\n",
        "        conv_size = []\n",
        "        input_size = SAMPLE_LENGTH\n",
        "        for i in range(depth):\n",
        "            conv_dim_reduction = kernel_sizes[i]-1-paddings[i]*2\n",
        "            assert (input_size - conv_dim_reduction) % (strides[i]*poolings[i]) == 0\n",
        "            conv_size.append(int((input_size - conv_dim_reduction)/strides[i]/poolings[i]))\n",
        "            input_size = conv_size[i]\n",
        "            print(\"Conv layer {} has output size {} and {} channels.\".format(i, conv_size[i], channels[i]))\n",
        "\n",
        "        # Calculate total size of the output of the convolutional layers.\n",
        "        conv_output_size = conv_size[-1]*channels[-1]\n",
        "        \n",
        "        self.module_list = nn.ModuleList()\n",
        "        in_channels = 1\n",
        "        for i in range(conv_depth):\n",
        "            conv = nn.Conv1d(in_channels=in_channels, out_channels=channels[i], kernel_size=kernel_sizes[i], stride=strides[i], padding=paddings[i])\n",
        "            in_channels = channels[i]\n",
        "            pool = nn.MaxPool1d(poolings[i])\n",
        "            batchnorm = nn.BatchNorm1d(channels[i]) if (conv_batch_norms[i]) else nn.Identity()\n",
        "            dropout = nn.Dropout(p=conv_dropouts[i])\n",
        "\n",
        "            self.module_list.append(nn.ModuleList([conv, pool, batchnorm, dropout]))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        for conv, pool, batchnorm, dropout in self.module_list:\n",
        "            x = conv(x)\n",
        "            x = pool(x)\n",
        "            x = nn_func.relu(x)\n",
        "            x = batchnorm(x)\n",
        "            x = dropout(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def output_size(self):\n",
        "        return conv_output_size\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, conv_mod):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        self.convs = conv_mod\n",
        "        \n",
        "        self.lins = nn.ModuleList()\n",
        "        in_features = conv_output_size\n",
        "        for i in range(lin_depth):\n",
        "            lin = nn.Linear(in_features=in_features, out_features=features[i])\n",
        "            in_features = features[i]\n",
        "            batchnorm = nn.BatchNorm1d(features[i]) if (lin_batch_norms[i]) else nn.Identity()\n",
        "            dropout = nn.Dropout(p=lin_dropouts[i])\n",
        "\n",
        "            self.lins.append(nn.ModuleList([lin, batchnorm, dropout]))\n",
        "\n",
        "        self.lin_out = nn.Linear(in_features=in_features, out_features=1)\n",
        "        \n",
        "        \n",
        "    def forward(self, samples, fft):\n",
        "        x = samples.unsqueeze(1)\n",
        "\n",
        "        x = self.convs(x)\n",
        "\n",
        "        x = x.flatten(1)\n",
        "\n",
        "        for lin, batchnorm, dropout in self.lins:\n",
        "            x = lin(x)\n",
        "            x = nn_func.relu(x)\n",
        "            x = batchnorm(x)\n",
        "            x = dropout(x)\n",
        "            \n",
        "        return self.lin_out(x).unsqueeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLAnanJZVLk0"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNgMtUn3VLk1"
      },
      "outputs": [],
      "source": [
        "conv_depth = 2\n",
        "kernel_sizes = [5, 5, 5, 5]\n",
        "signal_conv = ConvMod(\n",
        "    depth = conv_depth,\n",
        "    channels = [8, 8, 8, 8], \n",
        "    kernel_sizes = kernel_sizes, \n",
        "    strides = [1 for _ in range(conv_depth)], \n",
        "    paddings = [int((kernel_size - 1)/2) for kernel_size in kernel_sizes], \n",
        "    poolings = [2,2,2,2], \n",
        "    batch_norms = [False for _ in range(conv_depth)], \n",
        "    dropouts = [0.0 for _ in range(conv_depth)])\n",
        "\n",
        "lin_depth = 2\n",
        "features = [256, 128]\n",
        "lin_batch_norms = [False for _ in range(lin_depth)]\n",
        "lin_dropouts = [0.0 for _ in range(lin_depth)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jboJuhoEVLk2"
      },
      "source": [
        "#### Parameter validation and processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE-6xqREVLk2"
      },
      "outputs": [],
      "source": [
        "assert len(channels) >= conv_depth\n",
        "assert len(kernel_sizes) >= conv_depth\n",
        "assert len(strides) >= conv_depth\n",
        "assert len(paddings) >= conv_depth\n",
        "assert len(poolings) >= conv_depth\n",
        "assert len(conv_batch_norms) >= conv_depth\n",
        "assert len(conv_dropouts) >= conv_depth\n",
        "for kernel_size in kernel_sizes:\n",
        "    assert kernel_size % 2 == 1, \"Only odd kernel sizes are supported.\"\n",
        "for dropout in conv_dropouts:\n",
        "    assert 0 <= dropout and dropout <= 1, \"Dropout must be between 0 and 1.\"\n",
        "\n",
        "conv_size = []\n",
        "input_size = SAMPLE_LENGTH\n",
        "for i in range(conv_depth):\n",
        "    conv_dim_reduction = kernel_sizes[i]-1-paddings[i]*2\n",
        "    assert (input_size - conv_dim_reduction) % (strides[i]*poolings[i]) == 0\n",
        "    conv_size.append(int((input_size - conv_dim_reduction)/strides[i]/poolings[i]))\n",
        "    input_size = conv_size[i]\n",
        "    print(\"Conv layer {} has output size {} and {} channels.\".format(i, conv_size[i], channels[i]))\n",
        "\n",
        "conv_output_size = conv_size[-1]*channels[-1]\n",
        "\n",
        "\n",
        "assert len(features) >= lin_depth\n",
        "assert len(lin_batch_norms) >= lin_depth\n",
        "assert len(lin_dropouts) >= lin_depth\n",
        "for dropout in lin_dropouts:\n",
        "    assert 0 <= dropout and dropout <= 1, \"Dropout must be between 0 and 1.\"\n",
        "for i in range(lin_depth):\n",
        "    print(\"Lin layer {} has output size {}.\".format(i, features[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAYW93NKVLk3"
      },
      "source": [
        "### Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aC9CSpPVLk3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "net = Net(signal_conv)\n",
        "if use_cuda:\n",
        "    net.cuda()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33PC3H1tVLk4"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLbmywdRVLk5"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 1e-5\n",
        "WEIGHT_DECAY = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLXdnp1sVLk5"
      },
      "outputs": [],
      "source": [
        "criterion = nn.MSELoss()  \n",
        "\n",
        "# weight_decay is equal to L2 regularization\n",
        "optimizer = optim.AdamW(net.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjCGW-AXVLk6"
      },
      "outputs": [],
      "source": [
        "def to_torch(x):\n",
        "    variable = Variable(torch.from_numpy(x))\n",
        "    if use_cuda:\n",
        "        variable = variable.cuda()\n",
        "    return variable\n",
        "\n",
        "def mean_cent_err(freq_map, output):\n",
        "    target_frequency = np.array(list(map(parameters.map_to_frequency, get_numpy(freq_map))))\n",
        "    output_frequency = np.array(list(map(parameters.map_to_frequency, get_numpy(output))))\n",
        "    return np.array([abs(aus.cent_diff(target_frequency, output_frequency)) for target_frequency, output_frequency in zip(target_frequency, output_frequency)]).mean()\n",
        "\n",
        "def test_net(net, validation_loader, criterion):\n",
        "    was_training = net.training\n",
        "    net.eval()\n",
        "    num_iterations = 100\n",
        "    total_loss = 0\n",
        "    total_cent_diff = 0\n",
        "    for signal, fft, freq_map in itertools.islice(validation_loader, num_iterations):\n",
        "        signal = signal.to(device)\n",
        "        fft = fft.to(device)\n",
        "        freq_map = freq_map.to(device)\n",
        "        output = net(signal, fft)\n",
        "        \n",
        "        total_loss += criterion(output, freq_map)\n",
        "        total_cent_diff += mean_cent_err(freq_map, output)\n",
        "    net.train(mode=was_training)\n",
        "    return total_loss.item()/num_iterations, total_cent_diff/num_iterations\n",
        "\n",
        "def manual_test(net, validation_parameters):\n",
        "    was_training = net.training\n",
        "    net.eval()\n",
        "    num_iterations = 5\n",
        "    for data_point in [validation_parameters.generate_at_index(i) for i in range(5) ]:\n",
        "        samples = to_torch(data_point.samples()).unsqueeze(0)\n",
        "        fft = to_torch(data_point.audio().fft()).unsqueeze(0)\n",
        "        freq_map = to_torch(np.array([data_point.frequency_map()])).unsqueeze(0)\n",
        "        output = net(samples, fft)\n",
        "        target_frequency = parameters.map_to_frequency(freq_map.item())\n",
        "        output_frequency = parameters.map_to_frequency(output.item())\n",
        "        print(\"Frequency: {:.2f} Output: {:.2f} Cent diff: {:.2f}\".format(target_frequency, output_frequency, aus.cent_diff(target_frequency, output_frequency)))\n",
        "    net.train(mode=was_training)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbqU-FdsVLk6"
      },
      "outputs": [],
      "source": [
        "NUM_BATCHES = 10000\n",
        "EVAL_EVERY = 1000\n",
        "LOG_EVERY = EVAL_EVERY\n",
        "\n",
        "\n",
        "train_log_losses = []\n",
        "train_diffs = []\n",
        "train_iter = []\n",
        "\n",
        "val_log_losses = []\n",
        "val_diffs = []\n",
        "val_iter = []\n",
        "\n",
        "net.train()\n",
        "for i, (signal, fft, frequency_map) in enumerate(itertools.islice(training_loader, NUM_BATCHES+1)):\n",
        "    if i%EVAL_EVERY == 0:\n",
        "        val_loss, val_cent_diff = test_net(net, validation_loader, criterion)\n",
        "        val_log_losses.append(np.log10(val_loss))\n",
        "        val_diffs.append(val_cent_diff)\n",
        "        val_iter.append(i)\n",
        "\n",
        "    signal = signal.to(device)\n",
        "    fft = fft.to(device)\n",
        "    frequency_map = frequency_map.to(device)\n",
        "    output = net(signal, fft)\n",
        "    loss = criterion(output, frequency_map)\n",
        "\n",
        "    \n",
        "    train_log_losses.append(np.log10(loss.item()))\n",
        "    train_diffs.append(mean_cent_err(frequency_map, output))\n",
        "    train_iter.append(i)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if i % LOG_EVERY == 0:\n",
        "        display.clear_output(wait=True)\n",
        "        fig = plt.figure(figsize=(12, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(train_iter, train_log_losses, label=\"Training Loss\")\n",
        "        plt.plot(val_iter, val_log_losses, label=\"Validation Loss\")\n",
        "        plt.xlim(0, NUM_BATCHES+1)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(train_iter, train_diffs, label=\"Training Cent Diff\")\n",
        "        plt.plot(val_iter, val_diffs, label=\"Validation Cent Diff\")\n",
        "        plt.axhline(y=5, color='r', linestyle='-')\n",
        "        plt.ylim(0, 500)\n",
        "        plt.xlim(0, NUM_BATCHES+1)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "val_loss, val_cent_diff = test_net(net, validation_loader, criterion)\n",
        "print(f\"{i}: Loss={val_loss}, Cent diff={val_cent_diff:.2f}\")\n",
        "\n",
        "manual_test(net, validation_parameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqiZY2yKVLk7"
      },
      "outputs": [],
      "source": [
        "manual_test(net, validation_generator)\n",
        "test_net(net, validation_loader, criterion)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
